#!/usr/bin/env python3
"""
NaNæŸå¤±è¯Šæ–­è„šæœ¬ - ç²¾ç¡®å®šä½é—®é¢˜æ‰€åœ¨
"""

import torch
import torch.nn as nn
import numpy as np
import sys
import os
from pathlib import Path

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°Pythonè·¯å¾„
project_root = Path(__file__).parent.parent
sys.path.insert(0, str(project_root))

from src.baseline.data.dataset import create_data_loaders
from src.baseline.models.personalized_generator import PersonalizedHeadlineGenerator
from src.baseline.utils.config import load_configs  # ä¿®æ­£å‡½æ•°å

def check_tensor_health(tensor, name="Tensor"):
    """æ£€æŸ¥tensorçš„æ•°å€¼å¥åº·çŠ¶å†µ"""
    if tensor is None:
        print(f"âŒ {name}: None")
        return False
    
    has_nan = torch.isnan(tensor).any()
    has_inf = torch.isinf(tensor).any()
    min_val = tensor.min().item() if not has_nan else "NaN"
    max_val = tensor.max().item() if not has_nan else "NaN"
    mean_val = tensor.mean().item() if not has_nan else "NaN"
    
    print(f"ğŸ” {name}: shape={tensor.shape}, dtype={tensor.dtype}")
    print(f"   èŒƒå›´: [{min_val}, {max_val}], å‡å€¼: {mean_val}")
    print(f"   NaN: {has_nan.item()}, Inf: {has_inf.item()}")
    
    if has_nan or has_inf:
        print(f"âŒ {name} åŒ…å«å¼‚å¸¸å€¼!")
        return False
    
    # æ£€æŸ¥æ¢¯åº¦èŒƒå›´æ˜¯å¦åˆç†
    if hasattr(tensor, 'grad') and tensor.grad is not None:
        grad_has_nan = torch.isnan(tensor.grad).any()
        grad_has_inf = torch.isinf(tensor.grad).any()
        grad_norm = tensor.grad.norm().item() if not grad_has_nan else "NaN"
        print(f"   æ¢¯åº¦: norm={grad_norm}, NaN: {grad_has_nan.item()}, Inf: {grad_has_inf.item()}")
        
        if grad_has_nan or grad_has_inf:
            print(f"âŒ {name} æ¢¯åº¦åŒ…å«å¼‚å¸¸å€¼!")
            return False
    
    print(f"âœ… {name} æ•°å€¼å¥åº·")
    return True

def diagnose_model_forward(model, batch, device):
    """è¯Šæ–­æ¨¡å‹å‰å‘ä¼ æ’­è¿‡ç¨‹"""
    print("\n=== ğŸ” æ¨¡å‹å‰å‘ä¼ æ’­è¯Šæ–­ ===")
    
    model.eval()
    
    # æ£€æŸ¥è¾“å…¥æ•°æ®
    print("\n1. æ£€æŸ¥è¾“å…¥æ•°æ®:")
    for key, value in batch.items():
        if isinstance(value, torch.Tensor):
            check_tensor_health(value, f"è¾“å…¥-{key}")
    
    try:
        # é€æ­¥æ‰§è¡Œå‰å‘ä¼ æ’­
        print("\n2. æ‰§è¡Œå‰å‘ä¼ æ’­...")
        
        with torch.no_grad():
            # ç”¨æˆ·ç¼–ç 
            print("   -> ç”¨æˆ·ç¼–ç é˜¶æ®µ")
            user_history = batch['user_history'].to(device)
            history_mask = batch.get('history_mask', None)
            if history_mask is not None:
                history_mask = history_mask.to(device)
            
            user_embedding = model.encode_user(user_history, history_mask)
            if not check_tensor_health(user_embedding, "ç”¨æˆ·åµŒå…¥"):
                return False
            
            # æ–°é—»ç¼–ç 
            print("   -> æ–°é—»ç¼–ç é˜¶æ®µ")
            news_input_ids = batch['news_input_ids'].to(device)
            news_sentence_positions = batch['news_sentence_positions'].to(device)
            news_attention_mask = batch.get('news_attention_mask', None)
            if news_attention_mask is not None:
                news_attention_mask = news_attention_mask.to(device)
            
            encoder_outputs = model.encode_news_body(
                news_input_ids, news_sentence_positions, news_attention_mask
            )
            if not check_tensor_health(encoder_outputs, "ç¼–ç å™¨è¾“å‡º"):
                return False
            
            print("âœ… å‰å‘ä¼ æ’­æ£€æŸ¥é€šè¿‡")
            return True
            
    except Exception as e:
        print(f"âŒ å‰å‘ä¼ æ’­å‡ºé”™: {str(e)}")
        import traceback
        traceback.print_exc()
        return False

def diagnose_loss_computation(model, batch, device):
    """è¯Šæ–­æŸå¤±è®¡ç®—è¿‡ç¨‹"""
    print("\n=== ğŸ” æŸå¤±è®¡ç®—è¯Šæ–­ ===")
    
    model.train()
    
    try:
        # å‡†å¤‡è¾“å…¥
        user_history = batch['user_history'].to(device)
        news_input_ids = batch['news_input_ids'].to(device)
        news_sentence_positions = batch['news_sentence_positions'].to(device)
        target_ids = batch['target_ids'].to(device)
        
        history_mask = batch.get('history_mask', None)
        if history_mask is not None:
            history_mask = history_mask.to(device)
        
        news_attention_mask = batch.get('news_attention_mask', None)
        if news_attention_mask is not None:
            news_attention_mask = news_attention_mask.to(device)
        
        print("1. æ‰§è¡Œå®Œæ•´å‰å‘ä¼ æ’­...")
        outputs = model(
            user_history=user_history,
            news_input_ids=news_input_ids,
            news_sentence_positions=news_sentence_positions,
            history_mask=history_mask,
            news_attention_mask=news_attention_mask,
            target_ids=target_ids,
            teacher_forcing_ratio=1.0
        )
        
        print("2. æ£€æŸ¥æ¨¡å‹è¾“å‡º...")
        for key, value in outputs.items():
            if isinstance(value, torch.Tensor):
                if not check_tensor_health(value, f"è¾“å‡º-{key}"):
                    return False
        
        print("3. è®¡ç®—æŸå¤±...")
        # æ£€æŸ¥logits
        if 'logits' in outputs:
            logits = outputs['logits']
            
            # é‡æ–°æ•´ç†logitså’Œtargets
            batch_size, seq_len, vocab_size = logits.shape
            logits_flat = logits.view(-1, vocab_size)
            targets_flat = target_ids[:, 1:].contiguous().view(-1)  # å»æ‰<SOS>
            
            print(f"   Logitså½¢çŠ¶: {logits_flat.shape}")
            print(f"   Targetså½¢çŠ¶: {targets_flat.shape}")
            
            if not check_tensor_health(logits_flat, "å±•å¹³Logits"):
                return False
            
            if not check_tensor_health(targets_flat, "å±•å¹³Targets"):
                return False
            
            # è®¡ç®—æŸå¤±
            loss_fn = nn.CrossEntropyLoss(ignore_index=0)  # å¿½ç•¥padding
            loss = loss_fn(logits_flat, targets_flat)
            
            if not check_tensor_health(loss, "æœ€ç»ˆæŸå¤±"):
                return False
            
            print(f"âœ… æŸå¤±è®¡ç®—æˆåŠŸ: {loss.item()}")
            return True
        else:
            print("âŒ æ¨¡å‹è¾“å‡ºä¸­æ²¡æœ‰æ‰¾åˆ°logits")
            return False
            
    except Exception as e:
        print(f"âŒ æŸå¤±è®¡ç®—å‡ºé”™: {str(e)}")
        import traceback
        traceback.print_exc()
        return False

def diagnose_nan_issue():
    """å®Œæ•´çš„NaNé—®é¢˜è¯Šæ–­"""
    print("ğŸš€ å¼€å§‹NaNé—®é¢˜è¯Šæ–­...")
    
    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    print(f"ä½¿ç”¨è®¾å¤‡: {device}")
    
    # 1. åŠ è½½é…ç½®
    print("\n=== ğŸ“‹ åŠ è½½é…ç½® ===")
    config_path = 'configs/baseline/pens_config.yaml'
    config = load_configs(config_path)
    print("âœ… é…ç½®åŠ è½½æˆåŠŸ")
    
    # 2. åŠ è½½è¯æ±‡è¡¨
    print("\n=== ğŸ“š åŠ è½½è¯æ±‡è¡¨ ===")
    import json
    with open('data/processed/vocab_cache.json', 'r', encoding='utf-8') as f:
        vocab = json.load(f)
    vocab_size = len(vocab)
    print(f"âœ… è¯æ±‡è¡¨åŠ è½½æˆåŠŸï¼Œå¤§å°: {vocab_size}")
    
    # 3. åˆ›å»ºæ•°æ®åŠ è½½å™¨ï¼ˆå°æ‰¹æ¬¡æµ‹è¯•ï¼‰
    print("\n=== ğŸ“Š åˆ›å»ºæµ‹è¯•æ•°æ®åŠ è½½å™¨ ===")
    try:
        # ç›´æ¥ä½¿ç”¨ç¡¬ç¼–ç çš„è·¯å¾„ï¼Œé¿å…é…ç½®é—®é¢˜
        train_loader, _, _ = create_data_loaders(
            train_path="data/processed/train_processed.pkl",
            valid_path="data/processed/valid_processed.pkl", 
            test_path="data/processed/test_processed.pkl",
            vocab=vocab,
            batch_size=2,  # ä½¿ç”¨å¾ˆå°çš„æ‰¹æ¬¡è¿›è¡Œæµ‹è¯•
            num_workers=0,
            max_title_length=20,
            max_body_length=100,
            max_user_history=2
        )
        print("âœ… æ•°æ®åŠ è½½å™¨åˆ›å»ºæˆåŠŸ")
    except Exception as e:
        print(f"âŒ æ•°æ®åŠ è½½å™¨åˆ›å»ºå¤±è´¥: {str(e)}")
        import traceback
        traceback.print_exc()
        return False
    
    # 4. åˆ›å»ºæ¨¡å‹
    print("\n=== ğŸ¤– åˆ›å»ºæ¨¡å‹ ===")
    try:
        model = PersonalizedHeadlineGenerator(
            vocab_size=vocab_size,
            user_encoder_config=config['model']['user_encoder'],
            transformer_config=config['model']['transformer'],
            decoder_config=config['model']['decoder']
        ).to(device)
        
        # ç»Ÿè®¡æ¨¡å‹å‚æ•°
        total_params = sum(p.numel() for p in model.parameters())
        trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)
        print(f"âœ… æ¨¡å‹åˆ›å»ºæˆåŠŸ")
        print(f"   æ€»å‚æ•°: {total_params:,}")
        print(f"   å¯è®­ç»ƒå‚æ•°: {trainable_params:,}")
        
    except Exception as e:
        print(f"âŒ æ¨¡å‹åˆ›å»ºå¤±è´¥: {str(e)}")
        import traceback
        traceback.print_exc()
        return False
    
    # 5. è·å–æµ‹è¯•æ‰¹æ¬¡
    print("\n=== ğŸ”¢ è·å–æµ‹è¯•æ‰¹æ¬¡ ===")
    try:
        batch = next(iter(train_loader))
        print(f"âœ… æµ‹è¯•æ‰¹æ¬¡è·å–æˆåŠŸï¼Œæ‰¹æ¬¡å¤§å°: {len(batch['user_history'])}")
    except Exception as e:
        print(f"âŒ è·å–æµ‹è¯•æ‰¹æ¬¡å¤±è´¥: {str(e)}")
        return False
    
    # 6. è¯Šæ–­å‰å‘ä¼ æ’­
    success = diagnose_model_forward(model, batch, device)
    if not success:
        print("âŒ å‰å‘ä¼ æ’­è¯Šæ–­å¤±è´¥")
        return False
    
    # 7. è¯Šæ–­æŸå¤±è®¡ç®—
    success = diagnose_loss_computation(model, batch, device)
    if not success:
        print("âŒ æŸå¤±è®¡ç®—è¯Šæ–­å¤±è´¥")
        return False
    
    print("\nğŸ‰ æ‰€æœ‰è¯Šæ–­æ£€æŸ¥é€šè¿‡ï¼")
    print("å»ºè®®æ£€æŸ¥ä»¥ä¸‹å¯èƒ½çš„é—®é¢˜æº:")
    print("1. å­¦ä¹ ç‡æ˜¯å¦ä»ç„¶è¿‡é«˜")
    print("2. æ‰¹æ¬¡å¤§å°æ˜¯å¦è¿‡å¤§")
    print("3. æ¢¯åº¦è£å‰ªæ˜¯å¦ç”Ÿæ•ˆ")
    print("4. æ˜¯å¦å­˜åœ¨ç‰¹æ®Šçš„æ•°æ®æ ·æœ¬å¯¼è‡´æ•°å€¼ä¸ç¨³å®š")
    
    return True

if __name__ == '__main__':
    success = diagnose_nan_issue()
    if not success:
        print("\nâŒ è¯Šæ–­å‘ç°é—®é¢˜ï¼Œéœ€è¦è¿›ä¸€æ­¥è°ƒè¯•")
        sys.exit(1)
    else:
        print("\nâœ… è¯Šæ–­å®Œæˆï¼Œæ¨¡å‹åŸºç¡€åŠŸèƒ½æ­£å¸¸")