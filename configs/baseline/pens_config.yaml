# PENS基线方法综合配置文件

# 数据路径配置
paths:
  raw_data: "data/raw"
  processed_data: "data/processed"
  embeddings: "data/embeddings"
  results: "results/baseline"
  logs: "logs/baseline"

# 数据文件路径（训练器需要的格式）
data:
  train_path: "data/processed/train_processed.pkl"
  valid_path: "data/processed/valid_processed.pkl"
  test_path: "data/processed/test_processed.pkl"

# 数据预处理配置
preprocessing:
  # 数据采样配置（用于减少内存使用）
  data_sample_ratio: 0.01
  max_samples_per_split: 5000
  max_pos_samples_per_user: 2
  max_neg_samples_per_user: 1
  negative_sample_ratio: 0.005
  include_negative_samples: true
  
  # 文本长度限制
  max_title_length: 20   # 减少标题长度
  max_body_length: 500   # 大幅减少正文长度
  max_user_history: 5    # 进一步减少用户历史长度
  
  # 词汇表配置
  vocab_size: 15000      
  min_word_freq: 2       # 提高最小词频

# 模型配置
model:
  # 极简NAML用户编码器配置
  user_encoder:
    embedding_dim: 64   # 进一步减少嵌入维度
    hidden_dim: 64
    max_history_length: 3
    num_filters: 20     # 大幅减少过滤器数量
    num_categories: 15
    dropout: 0.3
  
  # 添加缺失的transformer配置
  transformer:
    d_model: 128        # transformer模型维度
    num_heads: 4        # 注意力头数 (自定义TransformerEncoder使用的参数名)
    num_layers: 2       # transformer层数
    d_ff: 256           # 前馈网络维度 (自定义TransformerEncoder使用的参数名)
    dropout: 0.1
    max_seq_length: 512
    max_sentence_length: 50
    d_sentence: 100
  
  # 添加缺失的decoder配置
  decoder:
    embedding_dim: 64
    hidden_dim: 128
    num_layers: 1
    dropout: 0.3
    max_decode_length: 15
  
  # 极简生成器配置
  generator:
    embedding_dim: 64
    hidden_dim: 128     # 减少隐藏层维度
    num_layers: 1       # 只用1层
    dropout: 0.3
    max_decode_length: 15  # 减少最大解码长度
  
  # 词嵌入配置
  word_embedding:
    dim: 128            
    pretrained: false

# 训练配置
training:
  # 简化的训练策略
  mode: "simple"  # 简化模式，不使用复杂的强化学习
  epochs: 3          # 减少训练轮数
  batch_size: 8      # 进一步减少批次大小
  learning_rate: 0.0001  # 大幅降低学习率，防止梯度爆炸
  weight_decay: 0.00001   # 降低权重衰减
  optimizer: "adam"
  
  # 设备配置
  device: "cuda"  
  num_workers: 0         # 使用单线程
  pin_memory: false
  gradient_clip: 0.5     # 降低梯度裁剪阈值
  
  # 评估配置
  eval_every: 100
  save_every: 500
  early_stopping_patience: 3
  
  # 日志配置
  log_level: "INFO"
  save_model: true

# 训练器需要的配置项
num_epochs: 3          # 减少训练轮数
batch_size: 8          # 统一批次大小，防止冲突
device: "cuda"         # 使用GPU
num_workers: 0         # 统一工作线程数
vocab_size: 15000      # 统一词汇表大小
min_word_freq: 2       # 提高最小词频
max_title_length: 20   # 减少标题长度
max_body_length: 100   # 大幅减少正文长度
max_user_history: 2    # 进一步减少用户历史长度
grad_clip: 0.5         # 统一梯度裁剪值
patience: 3
save_every: 5
teacher_forcing_ratio: 1.0

# 优化器配置
optimizer:
  lr: 0.0001             # 大幅降低学习率
  user_encoder_lr: 0.0001  # 用户编码器使用更小的学习率
  weight_decay: 0.00001     # 降低权重衰减
  eps: 0.0000001             # 改善数值稳定性

# 学习率调度器配置
scheduler:
  type: "linear"
  warmup_ratio: 0.1

# 检查点配置
checkpoint:
  dir: "results/baseline/checkpoints"
  max_keep: 3

# 评估配置
evaluation:
  metrics:
    - "rouge_1"
    - "rouge_2" 
    - "rouge_l"
    - "bleu"
  batch_size: 8